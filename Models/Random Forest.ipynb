{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12c1f84",
   "metadata": {},
   "source": [
    "In this notebook, we explore the decision tree model and tune the hyperparameters. We work with two different training sets, each with the a different subset of features. \n",
    "- train_1 contains features which were found by scraping a website of all food-related words.\n",
    "- train_2 contains features which were found by picking ingredients which occured at least 50 times in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d829253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "266f0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data\n",
    "#train_0 = pd.read_csv(\"../Data/original_data.csv\") # This line runs very slowly on my computer and using train_0 always leads to python kernel crashing\n",
    "train_1 = pd.read_csv(\"../Data/key_words_data.csv\")\n",
    "train_2 = pd.read_csv(\"../Data/train_trimmed.csv\")\n",
    "\n",
    "train_list = { 2:train_2} #Do one at a time in case the kernel crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba094c5",
   "metadata": {},
   "source": [
    "## Tuning the depth and n_estimators of a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb66888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set 2 . Done with parameters  (200, 60) . Accuracy =  0.7018655003542525\n",
      "Train set 2 . Done with parameters  (200, 65) . Accuracy =  0.7033740962135042\n",
      "Train set 2 . Done with parameters  (200, 70) . Accuracy =  0.7014128978900837\n",
      "Train set 2 . Done with parameters  (200, 75) . Accuracy =  0.7028209091022595\n"
     ]
    }
   ],
   "source": [
    "# Range of all hyperparameters\n",
    "max_depth_list = range(200,201,30)\n",
    "n_estimators_list = range(60,80,5)\n",
    "hyperparameter_list = product(max_depth_list,n_estimators_list)\n",
    "\n",
    "# List of all KPI metrics\n",
    "cv_accuracy_score = {i:{} for i in train_list}\n",
    "cv_recall_score = {i:{} for i in train_list}\n",
    "cv_f1_score = {i:{} for i in train_list}\n",
    "\n",
    "## Make stratified k fold splits of the data\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=429)\n",
    "\n",
    "## Use cross-validation to tune the hyperparameters\n",
    "for i,train in train_list.items(): \n",
    "    X_train = train.drop(columns=[\"id\",\"cuisine\"])\n",
    "    y_train = train[\"cuisine\"]\n",
    "    \n",
    "    for hyperparameter in hyperparameter_list: \n",
    "        max_depth, n_estimators = hyperparameter\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "        \n",
    "        ## To store KPI metrics for each of the 5 cross-validation sets\n",
    "        accuracy_list = []\n",
    "        f1_list = []\n",
    "        recall_list = []\n",
    "        \n",
    "        for train_idx, test_idx in skf.split(X_train,y_train):\n",
    "            X_train_train, y_train_train = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "            X_holdout, y_holdout = X_train.iloc[test_idx], y_train.iloc[test_idx]\n",
    "            clf.fit(X_train_train,y_train_train)\n",
    "            prediction = clf.predict(X_holdout)\n",
    "            accuracy_list.append(accuracy_score(y_holdout,prediction))\n",
    "            f1_list.append(f1_score(y_holdout,prediction,average=\"weighted\"))\n",
    "            recall_list.append(recall_score(y_holdout,prediction,average=\"weighted\"))\n",
    "            \n",
    "        cv_accuracy_score[i][hyperparameter] = np.mean(accuracy_list)\n",
    "        cv_f1_score[i][hyperparameter] = np.mean(f1_list)\n",
    "        cv_recall_score[i][hyperparameter] = np.mean(recall_list)\n",
    "\n",
    "        print(\"Train set\",i,\". Done with parameters \", hyperparameter, \n",
    "              \". Accuracy = \", cv_accuracy_score[i][hyperparameter]) #Output to keep track of the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2258df",
   "metadata": {},
   "source": [
    "## Results\n",
    "Due to computational intensity, we tuned the hyperparameters by fixing one and varying the other. We did this repeatedly for a few times to arrive at the near-optimal values of: \n",
    "- train_1 - max_depth=50, n_estimators=75 gives an accuracy of 71%\n",
    "- train_2 - max_depth=200, n_estimators=65 gives 70.3% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
